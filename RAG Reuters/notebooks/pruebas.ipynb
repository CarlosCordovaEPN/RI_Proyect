{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import umap.umap_ as umap\n",
    "\n",
    "class VectorizationEvaluator:\n",
    "    def __init__(self, documents, stop_words=None):\n",
    "        \"\"\"\n",
    "        Inicialización del evaluador de vectorización\n",
    "        \n",
    "        Args:\n",
    "        - documents: Lista de documentos (texto plano)\n",
    "        - stop_words: Conjunto de palabras vacías\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.stop_words = stop_words or set(stopwords.words('spanish'))\n",
    "    \n",
    "    def vectorize(self, method='bow', **kwargs):\n",
    "        \"\"\"\n",
    "        Vectorizar documentos usando diferentes métodos\n",
    "        \n",
    "        Args:\n",
    "        - method: Método de vectorización ('bow', 'tfidf', 'word2vec')\n",
    "        - kwargs: Parámetros adicionales para cada método\n",
    "        \n",
    "        Returns:\n",
    "        - Matriz de vectores\n",
    "        - Vocabulario\n",
    "        \"\"\"\n",
    "        if method == 'bow':\n",
    "            vectorizer = CountVectorizer(\n",
    "                stop_words=list(self.stop_words),\n",
    "                max_features=kwargs.get('max_features', 1000)\n",
    "            )\n",
    "            matrix = vectorizer.fit_transform(self.documents).toarray()\n",
    "            vocab = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        elif method == 'tfidf':\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                stop_words=list(self.stop_words),\n",
    "                max_features=kwargs.get('max_features', 1000)\n",
    "            )\n",
    "            matrix = vectorizer.fit_transform(self.documents).toarray()\n",
    "            vocab = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        elif method == 'word2vec':\n",
    "            # Tokenizar documentos\n",
    "            tokenized_docs = [\n",
    "                nltk.word_tokenize(doc.lower()) \n",
    "                for doc in self.documents\n",
    "            ]\n",
    "            \n",
    "            # Entrenar modelo Word2Vec\n",
    "            model = Word2Vec(\n",
    "                sentences=tokenized_docs, \n",
    "                vector_size=kwargs.get('vector_size', 100), \n",
    "                window=kwargs.get('window', 5), \n",
    "                min_count=kwargs.get('min_count', 1)\n",
    "            )\n",
    "            \n",
    "            # Calcular vector promedio por documento\n",
    "            matrix = []\n",
    "            for doc in tokenized_docs:\n",
    "                doc_vector = np.zeros(model.vector_size)\n",
    "                count = 0\n",
    "                for word in doc:\n",
    "                    if word in model.wv.key_to_index:\n",
    "                        doc_vector += model.wv[word]\n",
    "                        count += 1\n",
    "                \n",
    "                # Promedio de vectores\n",
    "                if count > 0:\n",
    "                    doc_vector /= count\n",
    "                \n",
    "                matrix.append(doc_vector)\n",
    "            \n",
    "            matrix = np.array(matrix)\n",
    "            vocab = list(model.wv.key_to_index.keys())\n",
    "        \n",
    "        return matrix, vocab\n",
    "    \n",
    "    def evaluate_similarity(self, matrix):\n",
    "        \"\"\"\n",
    "        Calcular similaridad de coseno entre documentos\n",
    "        \n",
    "        Args:\n",
    "        - matrix: Matriz de vectores\n",
    "        \n",
    "        Returns:\n",
    "        - Matriz de similaridad\n",
    "        \"\"\"\n",
    "        return cosine_similarity(matrix)\n",
    "    \n",
    "    def visualize_embeddings(self, matrix, method='pca'):\n",
    "        \"\"\"\n",
    "        Visualizar embeddings en 2D\n",
    "        \n",
    "        Args:\n",
    "        - matrix: Matriz de vectores\n",
    "        - method: Método de reducción de dimensionalidad\n",
    "        \n",
    "        Returns:\n",
    "        - Coordenadas 2D\n",
    "        \"\"\"\n",
    "        if method == 'pca':\n",
    "            reducer = PCA(n_components=2)\n",
    "        elif method == 'umap':\n",
    "            reducer = umap.UMAP(n_components=2)\n",
    "        \n",
    "        # Reducir dimensionalidad\n",
    "        coords = reducer.fit_transform(matrix)\n",
    "        return coords\n",
    "    \n",
    "    def analyze_vocabulary(self, vocab):\n",
    "        \"\"\"\n",
    "        Analizar características del vocabulario\n",
    "        \n",
    "        Args:\n",
    "        - vocab: Vocabulario de términos\n",
    "        \n",
    "        Returns:\n",
    "        - Diccionario con estadísticas del vocabulario\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'total_terms': len(vocab),\n",
    "            'term_lengths': [len(term) for term in vocab],\n",
    "            'avg_term_length': np.mean([len(term) for term in vocab]),\n",
    "            'unique_terms': len(set(vocab))\n",
    "        }\n",
    "    \n",
    "    def comprehensive_evaluation(self, methods=['bow', 'tfidf', 'word2vec']):\n",
    "        \"\"\"\n",
    "        Evaluación completa de métodos de vectorización\n",
    "        \n",
    "        Args:\n",
    "        - methods: Lista de métodos a evaluar\n",
    "        \n",
    "        Returns:\n",
    "        - Resultados de evaluación\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for method in methods:\n",
    "            # Vectorizar\n",
    "            matrix, vocab = self.vectorize(method)\n",
    "            \n",
    "            # Similaridad\n",
    "            similarity_matrix = self.evaluate_similarity(matrix)\n",
    "            \n",
    "            # Visualización\n",
    "            coords = self.visualize_embeddings(matrix, 'umap')\n",
    "            \n",
    "            # Análisis de vocabulario\n",
    "            vocab_stats = self.analyze_vocabulary(vocab)\n",
    "            \n",
    "            # Guardar resultados\n",
    "            results[method] = {\n",
    "                'matrix': matrix,\n",
    "                'vocab': vocab,\n",
    "                'similarity_matrix': similarity_matrix,\n",
    "                'embedding_coords': coords,\n",
    "                'vocab_stats': vocab_stats\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_results(self, results):\n",
    "        \"\"\"\n",
    "        Graficar resultados de evaluación\n",
    "        \n",
    "        Args:\n",
    "        - results: Resultados de evaluación\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Gráfico de dispersión de embeddings\n",
    "        plt.subplot(131)\n",
    "        for method, data in results.items():\n",
    "            plt.scatter(\n",
    "                data['embedding_coords'][:, 0], \n",
    "                data['embedding_coords'][:, 1], \n",
    "                label=method\n",
    "            )\n",
    "        plt.title('Embeddings de Documentos')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Heatmap de similaridad\n",
    "        plt.subplot(132)\n",
    "        sns.heatmap(\n",
    "            results['tfidf']['similarity_matrix'], \n",
    "            cmap='YlGnBu', \n",
    "            annot=True, \n",
    "            fmt='.2f'\n",
    "        )\n",
    "        plt.title('Matriz de Similaridad')\n",
    "        \n",
    "        # Estadísticas de vocabulario\n",
    "        plt.subplot(133)\n",
    "        vocab_stats = [\n",
    "            results[method]['vocab_stats']['total_terms'] \n",
    "            for method in results.keys()\n",
    "        ]\n",
    "        plt.bar(results.keys(), vocab_stats)\n",
    "        plt.title('Términos por Método')\n",
    "        plt.ylabel('Número de Términos')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Ejemplo de uso\n",
    "documentos = [\n",
    "    \"El análisis de datos es fundamental en ciencia\",\n",
    "    \"La inteligencia artificial transforma la tecnología\",\n",
    "    \"Los modelos de machine learning son complejos\",\n",
    "    \"La estadística ayuda a comprender los datos\",\n",
    "    \"La ciencia de datos combina múltiples disciplinas\"\n",
    "]\n",
    "\n",
    "# Inicializar evaluador\n",
    "evaluador = VectorizationEvaluator(documentos)\n",
    "\n",
    "# Realizar evaluación completa\n",
    "resultados = evaluador.comprehensive_evaluation()\n",
    "\n",
    "# Imprimir resultados detallados\n",
    "for metodo, datos in resultados.items():\n",
    "    print(f\"\\n--- Resultados para {metodo.upper()} ---\")\n",
    "    print(\"Estadísticas de Vocabulario:\")\n",
    "    for stat, valor in datos['vocab_stats'].items():\n",
    "        print(f\"{stat}: {valor}\")\n",
    "    \n",
    "    print(\"\\nMatriz de Vectores (primeras 2 filas):\")\n",
    "    print(datos['matrix'][:2])\n",
    "\n",
    "# Graficar resultados\n",
    "evaluador.plot_results(resultados)\n",
    "\n",
    "# Análisis comparativo detallado\n",
    "def analisis_comparativo(resultados):\n",
    "    print(\"\\n=== ANÁLISIS COMPARATIVO DE TÉCNICAS DE VECTORIZACIÓN ===\")\n",
    "    \n",
    "    # Comparación de dimensionalidad\n",
    "    print(\"\\n1. Dimensionalidad:\")\n",
    "    for metodo, datos in resultados.items():\n",
    "        print(f\"{metodo.upper()}: {datos['matrix'].shape}\")\n",
    "    \n",
    "    # Análisis de similaridad\n",
    "    print(\"\\n2. Similaridad entre Documentos:\")\n",
    "    for metodo, datos in resultados.items():\n",
    "        sim_matrix = datos['similarity_matrix']\n",
    "        print(f\"\\n{metodo.upper()}:\")\n",
    "        print(f\"  Promedio de similaridad: {np.mean(sim_matrix):.4f}\")\n",
    "        print(f\"  Máxima similaridad: {np.max(sim_matrix):.4f}\")\n",
    "        print(f\"  Mínima similaridad: {np.min(sim_matrix):.4f}\")\n",
    "    \n",
    "    # Análisis de vocabulario\n",
    "    print(\"\\n3. Características del Vocabulario:\")\n",
    "    for metodo, datos in resultados.items():\n",
    "        vocab_stats = datos['vocab_stats']\n",
    "        print(f\"\\n{metodo.upper()}:\")\n",
    "        print(f\"  Términos totales: {vocab_stats['total_terms']}\")\n",
    "        print(f\"  Longitud media de términos: {vocab_stats['avg_term_length']:.2f}\")\n",
    "\n",
    "# Ejecutar análisis comparativo\n",
    "analisis_comparativo(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Asegúrate de tener estas descargas\n",
    "nltk.download('punkt')\n",
    "\n",
    "class TextVectorizer:\n",
    "    def __init__(self, documents, stop_words=None):\n",
    "        \"\"\"\n",
    "        Inicialización del vectorizador de texto\n",
    "        \n",
    "        Args:\n",
    "        - documents: Lista de documentos (texto plano)\n",
    "        - stop_words: Conjunto de palabras vacías\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.stop_words = stop_words or set(stopwords.words('spanish'))\n",
    "    \n",
    "    def bag_of_words(self, max_features=1000):\n",
    "        \"\"\"\n",
    "        Vectorización usando Bag of Words\n",
    "        \n",
    "        Args:\n",
    "        - max_features: Número máximo de características\n",
    "        \n",
    "        Returns:\n",
    "        - Matriz de vectores BoW\n",
    "        - Vocabulario\n",
    "        \"\"\"\n",
    "        # Configurar vectorizador\n",
    "        vectorizer = CountVectorizer(\n",
    "            stop_words=list(self.stop_words),\n",
    "            max_features=max_features\n",
    "        )\n",
    "        \n",
    "        # Transformar documentos\n",
    "        bow_matrix = vectorizer.fit_transform(self.documents)\n",
    "        \n",
    "        return {\n",
    "            'matrix': bow_matrix.toarray(),\n",
    "            'vocabulary': vectorizer.get_feature_names_out(),\n",
    "            'vectorizer': vectorizer\n",
    "        }\n",
    "    \n",
    "    def tf_idf(self, max_features=1000):\n",
    "        \"\"\"\n",
    "        Vectorización usando TF-IDF\n",
    "        \n",
    "        Args:\n",
    "        - max_features: Número máximo de características\n",
    "        \n",
    "        Returns:\n",
    "        - Matriz de vectores TF-IDF\n",
    "        - Vocabulario\n",
    "        \"\"\"\n",
    "        # Configurar vectorizador TF-IDF\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            stop_words=list(self.stop_words),\n",
    "            max_features=max_features\n",
    "        )\n",
    "        \n",
    "        # Transformar documentos\n",
    "        tfidf_matrix = vectorizer.fit_transform(self.documents)\n",
    "        \n",
    "        return {\n",
    "            'matrix': tfidf_matrix.toarray(),\n",
    "            'vocabulary': vectorizer.get_feature_names_out(),\n",
    "            'vectorizer': vectorizer\n",
    "        }\n",
    "    \n",
    "    def word2vec(self, vector_size=100, window=5, min_count=1):\n",
    "        \"\"\"\n",
    "        Vectorización usando Word2Vec\n",
    "        \n",
    "        Args:\n",
    "        - vector_size: Dimensionalidad de los vectores\n",
    "        - window: Contexto de palabras\n",
    "        - min_count: Frecuencia mínima de palabras\n",
    "        \n",
    "        Returns:\n",
    "        - Modelo Word2Vec\n",
    "        - Matriz de vectores promedio de documentos\n",
    "        \"\"\"\n",
    "        # Tokenizar documentos\n",
    "        tokenized_docs = [\n",
    "            nltk.word_tokenize(doc.lower()) \n",
    "            for doc in self.documents\n",
    "        ]\n",
    "        \n",
    "        # Entrenar modelo Word2Vec\n",
    "        model = Word2Vec(\n",
    "            sentences=tokenized_docs, \n",
    "            vector_size=vector_size, \n",
    "            window=window, \n",
    "            min_count=min_count\n",
    "        )\n",
    "        \n",
    "        # Calcular vector promedio por documento\n",
    "        doc_vectors = []\n",
    "        for doc in tokenized_docs:\n",
    "            doc_vector = np.zeros(vector_size)\n",
    "            count = 0\n",
    "            for word in doc:\n",
    "                if word in model.wv.key_to_index:\n",
    "                    doc_vector += model.wv[word]\n",
    "                    count += 1\n",
    "            \n",
    "            # Promedio de vectores\n",
    "            if count > 0:\n",
    "                doc_vector /= count\n",
    "            \n",
    "            doc_vectors.append(doc_vector)\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'doc_vectors': np.array(doc_vectors)\n",
    "        }\n",
    "\n",
    "# Ejemplo de uso\n",
    "documentos = [\n",
    "    \"El gato duerme en el sofá\",\n",
    "    \"El perro juega en el jardín\",\n",
    "    \"El gato y el perro son animales domésticos\"\n",
    "]\n",
    "\n",
    "# Inicializar vectorizador\n",
    "vectorizador = TextVectorizer(documentos)\n",
    "\n",
    "# Bag of Words\n",
    "bow_result = vectorizador.bag_of_words()\n",
    "print(\"\\n--- Bag of Words ---\")\n",
    "print(\"Matriz BoW:\")\n",
    "print(bow_result['matrix'])\n",
    "print(\"\\nVocabulario BoW:\")\n",
    "print(bow_result['vocabulary'])\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_result = vectorizador.tf_idf()\n",
    "print(\"\\n--- TF-IDF ---\")\n",
    "print(\"Matriz TF-IDF:\")\n",
    "print(tfidf_result['matrix'])\n",
    "print(\"\\nVocabulario TF-IDF:\")\n",
    "print(tfidf_result['vocabulary'])\n",
    "\n",
    "# Word2Vec\n",
    "word2vec_result = vectorizador.word2vec()\n",
    "print(\"\\n--- Word2Vec ---\")\n",
    "print(\"Vectores de documentos:\")\n",
    "print(word2vec_result['doc_vectors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\carlo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\carlo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\carlo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocesamiento completado. Archivo guardado en: reuters_preprocessed_general.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Ruta al corpus y archivo de stop words\n",
    "corpus_dir = \"../data/reuters/test\"  # Cambia esto según tu sistema\n",
    "stopwords_file = \"../data/reuters/stopwords.txt\"  # Cambia esto según tu sistema\n",
    "\n",
    "# Cargar las stop words personalizadas\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        custom_stopwords = set(f.read().splitlines())\n",
    "    # Combinar con las stop words predeterminadas de NLTK\n",
    "    nltk_stopwords = set(stopwords.words('english'))\n",
    "    return custom_stopwords.union(nltk_stopwords)\n",
    "\n",
    "stop_words = load_stopwords(stopwords_file)\n",
    "\n",
    "# Inicializar lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocesamiento de texto\n",
    "def preprocess_text(text, stop_words):\n",
    "    # Normalización: convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Eliminación de caracteres no deseados\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Eliminar puntuaciones\n",
    "    text = re.sub(r'\\d+', '', text)  # Eliminar números\n",
    "    \n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Eliminar stop words y lematizar\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(token) for token in tokens \n",
    "        if token.isalpha() and token not in stop_words\n",
    "    ]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Intentar leer un archivo \"tipo archivo\"\n",
    "def read_file(file_path):\n",
    "    try:\n",
    "        # Intentar leer como texto\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        # Si no es texto, intentar leer como binario y convertir a texto\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return f.read().decode('utf-8', errors='ignore')\n",
    "\n",
    "# Procesar documentos del corpus\n",
    "def process_corpus(corpus_dir, stop_words):\n",
    "    data = []\n",
    "    \n",
    "    for file_name in os.listdir(corpus_dir):\n",
    "        file_path = os.path.join(corpus_dir, file_name)\n",
    "        \n",
    "        # Ignorar si no es un archivo\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "        \n",
    "        content = read_file(file_path)\n",
    "        \n",
    "        # Extraer título y cuerpo\n",
    "        parts = content.split(\"\\n\", 1)\n",
    "        title = parts[0] if len(parts) > 0 else \"\"\n",
    "        body = parts[1] if len(parts) > 1 else \"\"\n",
    "        \n",
    "        # Preprocesar título y cuerpo\n",
    "        preprocessed_title = preprocess_text(title, stop_words)\n",
    "        preprocessed_body = preprocess_text(body, stop_words)\n",
    "        \n",
    "        # Guardar resultado\n",
    "        data.append({\"file_name\": file_name, \"title\": preprocessed_title, \"body\": preprocessed_body})\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Procesar el corpus y guardar el resultado\n",
    "processed_data = process_corpus(corpus_dir, stop_words)\n",
    "output_file = \"reuters_preprocessed_general.csv\"\n",
    "processed_data.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Preprocesamiento completado. Archivo guardado en: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primer documento procesado y tokenizado:\n",
      "['shower', 'continued', 'week', 'bahia', 'cocoa', 'zone', 'alleviating', 'drought', 'early', 'january', 'improving', 'prospect', 'coming', 'temporao', 'normal', 'humidity', 'level', 'restored', 'comissaria', 'smith', 'weekly', 'review', 'dry', 'period', 'mean', 'temporao', 'late', 'year', 'arrival', 'week', 'ended', 'february', 'bag', 'kilo', 'making', 'cumulative', 'total', 'season', 'mln', 'stage', 'year', 'cocoa', 'delivered', 'earlier', 'consignment', 'included', 'arrival', 'figure', 'comissaria', 'smith', 'doubt', 'crop', 'cocoa', 'harvesting', 'practically', 'end', 'total', 'bahia', 'crop', 'estimate', 'mln', 'bag', 'sale', 'standing', 'mln', 'hundred', 'thousand', 'bag', 'hand', 'farmer', 'middleman', 'exporter', 'processor', 'doubt', 'cocoa', 'fit', 'export', 'shipper', 'experiencing', 'dificulties', 'obtaining', 'bahia', 'superior', 'certificate', 'view', 'lower', 'quality', 'recent', 'week', 'farmer', 'sold', 'good', 'part', 'cocoa', 'held', 'consignment', 'comissaria', 'smith', 'spot', 'bean', 'price', 'rose', 'cruzados', 'arroba', 'kilo', 'bean', 'shipper', 'reluctant', 'offer', 'nearby', 'shipment', 'limited', 'sale', 'booked', 'march', 'shipment', 'dlrs', 'tonne', 'port', 'named', 'crop', 'sale', 'light', 'open', 'port', 'junejuly', 'dlrs', 'dlrs', 'york', 'july', 'augsept', 'dlrs', 'tonne', 'fob', 'routine', 'sale', 'butter', 'made', 'marchapril', 'sold', 'dlrs', 'aprilmay', 'butter', 'time', 'york', 'junejuly', 'dlrs', 'augsept', 'dlrs', 'time', 'york', 'sept', 'octdec', 'dlrs', 'time', 'york', 'dec', 'comissaria', 'smith', 'destination', 'covertible', 'currency', 'area', 'uruguay', 'open', 'port', 'cake', 'sale', 'registered', 'dlrs', 'marchapril', 'dlrs', 'dlrs', 'aug', 'time', 'york', 'dec', 'octdec', 'buyer', 'argentina', 'uruguay', 'convertible', 'currency', 'area', 'liquor', 'sale', 'limited', 'marchapril', 'selling', 'dlrs', 'junejuly', 'dlrs', 'time', 'york', 'july', 'augsept', 'dlrs', 'time', 'york', 'sept', 'octdec', 'time', 'york', 'dec', 'comissaria', 'smith', 'total', 'bahia', 'sale', 'estimated', 'mln', 'bag', 'crop', 'mln', 'bag', 'crop', 'final', 'figure', 'period', 'february', 'expected', 'published', 'brazilian', 'cocoa', 'trade', 'commission', 'carnival', 'end', 'midday', 'february']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar el primer documento procesado\n",
    "first_document = processed_data.iloc[0]['body']\n",
    "tokens = word_tokenize(first_document)\n",
    "\n",
    "print(\"Primer documento procesado y tokenizado:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
