{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import umap.umap_ as umap\n",
    "\n",
    "class VectorizationEvaluator:\n",
    "    def __init__(self, documents, stop_words=None):\n",
    "        \"\"\"\n",
    "        Inicialización del evaluador de vectorización\n",
    "        \n",
    "        Args:\n",
    "        - documents: Lista de documentos (texto plano)\n",
    "        - stop_words: Conjunto de palabras vacías\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.stop_words = stop_words or set(stopwords.words('spanish'))\n",
    "    \n",
    "    def vectorize(self, method='bow', **kwargs):\n",
    "        \"\"\"\n",
    "        Vectorizar documentos usando diferentes métodos\n",
    "        \n",
    "        Args:\n",
    "        - method: Método de vectorización ('bow', 'tfidf', 'word2vec')\n",
    "        - kwargs: Parámetros adicionales para cada método\n",
    "        \n",
    "        Returns:\n",
    "        - Matriz de vectores\n",
    "        - Vocabulario\n",
    "        \"\"\"\n",
    "        if method == 'bow':\n",
    "            vectorizer = CountVectorizer(\n",
    "                stop_words=list(self.stop_words),\n",
    "                max_features=kwargs.get('max_features', 1000)\n",
    "            )\n",
    "            matrix = vectorizer.fit_transform(self.documents).toarray()\n",
    "            vocab = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        elif method == 'tfidf':\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                stop_words=list(self.stop_words),\n",
    "                max_features=kwargs.get('max_features', 1000)\n",
    "            )\n",
    "            matrix = vectorizer.fit_transform(self.documents).toarray()\n",
    "            vocab = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        elif method == 'word2vec':\n",
    "            # Tokenizar documentos\n",
    "            tokenized_docs = [\n",
    "                nltk.word_tokenize(doc.lower()) \n",
    "                for doc in self.documents\n",
    "            ]\n",
    "            \n",
    "            # Entrenar modelo Word2Vec\n",
    "            model = Word2Vec(\n",
    "                sentences=tokenized_docs, \n",
    "                vector_size=kwargs.get('vector_size', 100), \n",
    "                window=kwargs.get('window', 5), \n",
    "                min_count=kwargs.get('min_count', 1)\n",
    "            )\n",
    "            \n",
    "            # Calcular vector promedio por documento\n",
    "            matrix = []\n",
    "            for doc in tokenized_docs:\n",
    "                doc_vector = np.zeros(model.vector_size)\n",
    "                count = 0\n",
    "                for word in doc:\n",
    "                    if word in model.wv.key_to_index:\n",
    "                        doc_vector += model.wv[word]\n",
    "                        count += 1\n",
    "                \n",
    "                # Promedio de vectores\n",
    "                if count > 0:\n",
    "                    doc_vector /= count\n",
    "                \n",
    "                matrix.append(doc_vector)\n",
    "            \n",
    "            matrix = np.array(matrix)\n",
    "            vocab = list(model.wv.key_to_index.keys())\n",
    "        \n",
    "        return matrix, vocab\n",
    "    \n",
    "    def evaluate_similarity(self, matrix):\n",
    "        \"\"\"\n",
    "        Calcular similaridad de coseno entre documentos\n",
    "        \n",
    "        Args:\n",
    "        - matrix: Matriz de vectores\n",
    "        \n",
    "        Returns:\n",
    "        - Matriz de similaridad\n",
    "        \"\"\"\n",
    "        return cosine_similarity(matrix)\n",
    "    \n",
    "    def visualize_embeddings(self, matrix, method='pca'):\n",
    "        \"\"\"\n",
    "        Visualizar embeddings en 2D\n",
    "        \n",
    "        Args:\n",
    "        - matrix: Matriz de vectores\n",
    "        - method: Método de reducción de dimensionalidad\n",
    "        \n",
    "        Returns:\n",
    "        - Coordenadas 2D\n",
    "        \"\"\"\n",
    "        if method == 'pca':\n",
    "            reducer = PCA(n_components=2)\n",
    "        elif method == 'umap':\n",
    "            reducer = umap.UMAP(n_components=2)\n",
    "        \n",
    "        # Reducir dimensionalidad\n",
    "        coords = reducer.fit_transform(matrix)\n",
    "        return coords\n",
    "    \n",
    "    def analyze_vocabulary(self, vocab):\n",
    "        \"\"\"\n",
    "        Analizar características del vocabulario\n",
    "        \n",
    "        Args:\n",
    "        - vocab: Vocabulario de términos\n",
    "        \n",
    "        Returns:\n",
    "        - Diccionario con estadísticas del vocabulario\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'total_terms': len(vocab),\n",
    "            'term_lengths': [len(term) for term in vocab],\n",
    "            'avg_term_length': np.mean([len(term) for term in vocab]),\n",
    "            'unique_terms': len(set(vocab))\n",
    "        }\n",
    "    \n",
    "    def comprehensive_evaluation(self, methods=['bow', 'tfidf', 'word2vec']):\n",
    "        \"\"\"\n",
    "        Evaluación completa de métodos de vectorización\n",
    "        \n",
    "        Args:\n",
    "        - methods: Lista de métodos a evaluar\n",
    "        \n",
    "        Returns:\n",
    "        - Resultados de evaluación\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for method in methods:\n",
    "            # Vectorizar\n",
    "            matrix, vocab = self.vectorize(method)\n",
    "            \n",
    "            # Similaridad\n",
    "            similarity_matrix = self.evaluate_similarity(matrix)\n",
    "            \n",
    "            # Visualización\n",
    "            coords = self.visualize_embeddings(matrix, 'umap')\n",
    "            \n",
    "            # Análisis de vocabulario\n",
    "            vocab_stats = self.analyze_vocabulary(vocab)\n",
    "            \n",
    "            # Guardar resultados\n",
    "            results[method] = {\n",
    "                'matrix': matrix,\n",
    "                'vocab': vocab,\n",
    "                'similarity_matrix': similarity_matrix,\n",
    "                'embedding_coords': coords,\n",
    "                'vocab_stats': vocab_stats\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_results(self, results):\n",
    "        \"\"\"\n",
    "        Graficar resultados de evaluación\n",
    "        \n",
    "        Args:\n",
    "        - results: Resultados de evaluación\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Gráfico de dispersión de embeddings\n",
    "        plt.subplot(131)\n",
    "        for method, data in results.items():\n",
    "            plt.scatter(\n",
    "                data['embedding_coords'][:, 0], \n",
    "                data['embedding_coords'][:, 1], \n",
    "                label=method\n",
    "            )\n",
    "        plt.title('Embeddings de Documentos')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Heatmap de similaridad\n",
    "        plt.subplot(132)\n",
    "        sns.heatmap(\n",
    "            results['tfidf']['similarity_matrix'], \n",
    "            cmap='YlGnBu', \n",
    "            annot=True, \n",
    "            fmt='.2f'\n",
    "        )\n",
    "        plt.title('Matriz de Similaridad')\n",
    "        \n",
    "        # Estadísticas de vocabulario\n",
    "        plt.subplot(133)\n",
    "        vocab_stats = [\n",
    "            results[method]['vocab_stats']['total_terms'] \n",
    "            for method in results.keys()\n",
    "        ]\n",
    "        plt.bar(results.keys(), vocab_stats)\n",
    "        plt.title('Términos por Método')\n",
    "        plt.ylabel('Número de Términos')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Ejemplo de uso\n",
    "documentos = [\n",
    "    \"El análisis de datos es fundamental en ciencia\",\n",
    "    \"La inteligencia artificial transforma la tecnología\",\n",
    "    \"Los modelos de machine learning son complejos\",\n",
    "    \"La estadística ayuda a comprender los datos\",\n",
    "    \"La ciencia de datos combina múltiples disciplinas\"\n",
    "]\n",
    "\n",
    "# Inicializar evaluador\n",
    "evaluador = VectorizationEvaluator(documentos)\n",
    "\n",
    "# Realizar evaluación completa\n",
    "resultados = evaluador.comprehensive_evaluation()\n",
    "\n",
    "# Imprimir resultados detallados\n",
    "for metodo, datos in resultados.items():\n",
    "    print(f\"\\n--- Resultados para {metodo.upper()} ---\")\n",
    "    print(\"Estadísticas de Vocabulario:\")\n",
    "    for stat, valor in datos['vocab_stats'].items():\n",
    "        print(f\"{stat}: {valor}\")\n",
    "    \n",
    "    print(\"\\nMatriz de Vectores (primeras 2 filas):\")\n",
    "    print(datos['matrix'][:2])\n",
    "\n",
    "# Graficar resultados\n",
    "evaluador.plot_results(resultados)\n",
    "\n",
    "# Análisis comparativo detallado\n",
    "def analisis_comparativo(resultados):\n",
    "    print(\"\\n=== ANÁLISIS COMPARATIVO DE TÉCNICAS DE VECTORIZACIÓN ===\")\n",
    "    \n",
    "    # Comparación de dimensionalidad\n",
    "    print(\"\\n1. Dimensionalidad:\")\n",
    "    for metodo, datos in resultados.items():\n",
    "        print(f\"{metodo.upper()}: {datos['matrix'].shape}\")\n",
    "    \n",
    "    # Análisis de similaridad\n",
    "    print(\"\\n2. Similaridad entre Documentos:\")\n",
    "    for metodo, datos in resultados.items():\n",
    "        sim_matrix = datos['similarity_matrix']\n",
    "        print(f\"\\n{metodo.upper()}:\")\n",
    "        print(f\"  Promedio de similaridad: {np.mean(sim_matrix):.4f}\")\n",
    "        print(f\"  Máxima similaridad: {np.max(sim_matrix):.4f}\")\n",
    "        print(f\"  Mínima similaridad: {np.min(sim_matrix):.4f}\")\n",
    "    \n",
    "    # Análisis de vocabulario\n",
    "    print(\"\\n3. Características del Vocabulario:\")\n",
    "    for metodo, datos in resultados.items():\n",
    "        vocab_stats = datos['vocab_stats']\n",
    "        print(f\"\\n{metodo.upper()}:\")\n",
    "        print(f\"  Términos totales: {vocab_stats['total_terms']}\")\n",
    "        print(f\"  Longitud media de términos: {vocab_stats['avg_term_length']:.2f}\")\n",
    "\n",
    "# Ejecutar análisis comparativo\n",
    "analisis_comparativo(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Asegúrate de tener estas descargas\n",
    "nltk.download('punkt')\n",
    "\n",
    "class TextVectorizer:\n",
    "    def __init__(self, documents, stop_words=None):\n",
    "        \"\"\"\n",
    "        Inicialización del vectorizador de texto\n",
    "        \n",
    "        Args:\n",
    "        - documents: Lista de documentos (texto plano)\n",
    "        - stop_words: Conjunto de palabras vacías\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.stop_words = stop_words or set(stopwords.words('spanish'))\n",
    "    \n",
    "    def bag_of_words(self, max_features=1000):\n",
    "        \"\"\"\n",
    "        Vectorización usando Bag of Words\n",
    "        \n",
    "        Args:\n",
    "        - max_features: Número máximo de características\n",
    "        \n",
    "        Returns:\n",
    "        - Matriz de vectores BoW\n",
    "        - Vocabulario\n",
    "        \"\"\"\n",
    "        # Configurar vectorizador\n",
    "        vectorizer = CountVectorizer(\n",
    "            stop_words=list(self.stop_words),\n",
    "            max_features=max_features\n",
    "        )\n",
    "        \n",
    "        # Transformar documentos\n",
    "        bow_matrix = vectorizer.fit_transform(self.documents)\n",
    "        \n",
    "        return {\n",
    "            'matrix': bow_matrix.toarray(),\n",
    "            'vocabulary': vectorizer.get_feature_names_out(),\n",
    "            'vectorizer': vectorizer\n",
    "        }\n",
    "    \n",
    "    def tf_idf(self, max_features=1000):\n",
    "        \"\"\"\n",
    "        Vectorización usando TF-IDF\n",
    "        \n",
    "        Args:\n",
    "        - max_features: Número máximo de características\n",
    "        \n",
    "        Returns:\n",
    "        - Matriz de vectores TF-IDF\n",
    "        - Vocabulario\n",
    "        \"\"\"\n",
    "        # Configurar vectorizador TF-IDF\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            stop_words=list(self.stop_words),\n",
    "            max_features=max_features\n",
    "        )\n",
    "        \n",
    "        # Transformar documentos\n",
    "        tfidf_matrix = vectorizer.fit_transform(self.documents)\n",
    "        \n",
    "        return {\n",
    "            'matrix': tfidf_matrix.toarray(),\n",
    "            'vocabulary': vectorizer.get_feature_names_out(),\n",
    "            'vectorizer': vectorizer\n",
    "        }\n",
    "    \n",
    "    def word2vec(self, vector_size=100, window=5, min_count=1):\n",
    "        \"\"\"\n",
    "        Vectorización usando Word2Vec\n",
    "        \n",
    "        Args:\n",
    "        - vector_size: Dimensionalidad de los vectores\n",
    "        - window: Contexto de palabras\n",
    "        - min_count: Frecuencia mínima de palabras\n",
    "        \n",
    "        Returns:\n",
    "        - Modelo Word2Vec\n",
    "        - Matriz de vectores promedio de documentos\n",
    "        \"\"\"\n",
    "        # Tokenizar documentos\n",
    "        tokenized_docs = [\n",
    "            nltk.word_tokenize(doc.lower()) \n",
    "            for doc in self.documents\n",
    "        ]\n",
    "        \n",
    "        # Entrenar modelo Word2Vec\n",
    "        model = Word2Vec(\n",
    "            sentences=tokenized_docs, \n",
    "            vector_size=vector_size, \n",
    "            window=window, \n",
    "            min_count=min_count\n",
    "        )\n",
    "        \n",
    "        # Calcular vector promedio por documento\n",
    "        doc_vectors = []\n",
    "        for doc in tokenized_docs:\n",
    "            doc_vector = np.zeros(vector_size)\n",
    "            count = 0\n",
    "            for word in doc:\n",
    "                if word in model.wv.key_to_index:\n",
    "                    doc_vector += model.wv[word]\n",
    "                    count += 1\n",
    "            \n",
    "            # Promedio de vectores\n",
    "            if count > 0:\n",
    "                doc_vector /= count\n",
    "            \n",
    "            doc_vectors.append(doc_vector)\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'doc_vectors': np.array(doc_vectors)\n",
    "        }\n",
    "\n",
    "# Ejemplo de uso\n",
    "documentos = [\n",
    "    \"El gato duerme en el sofá\",\n",
    "    \"El perro juega en el jardín\",\n",
    "    \"El gato y el perro son animales domésticos\"\n",
    "]\n",
    "\n",
    "# Inicializar vectorizador\n",
    "vectorizador = TextVectorizer(documentos)\n",
    "\n",
    "# Bag of Words\n",
    "bow_result = vectorizador.bag_of_words()\n",
    "print(\"\\n--- Bag of Words ---\")\n",
    "print(\"Matriz BoW:\")\n",
    "print(bow_result['matrix'])\n",
    "print(\"\\nVocabulario BoW:\")\n",
    "print(bow_result['vocabulary'])\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_result = vectorizador.tf_idf()\n",
    "print(\"\\n--- TF-IDF ---\")\n",
    "print(\"Matriz TF-IDF:\")\n",
    "print(tfidf_result['matrix'])\n",
    "print(\"\\nVocabulario TF-IDF:\")\n",
    "print(tfidf_result['vocabulary'])\n",
    "\n",
    "# Word2Vec\n",
    "word2vec_result = vectorizador.word2vec()\n",
    "print(\"\\n--- Word2Vec ---\")\n",
    "print(\"Vectores de documentos:\")\n",
    "print(word2vec_result['doc_vectors'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
